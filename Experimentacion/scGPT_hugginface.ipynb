{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno, vamos a probar a usar el modelo scGPT a través de la plataforma HugginFace. Trataremos de seguir con este los tutoriales ofrecidos por los investigadores, aunque estos se realizaron pensado en la instalación completa del modelo. Veremos hasta que punto es compatible usar el modelo a través de Huggin Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "from tdc.multi_pred.anndata_dataset import DataLoader\n",
    "from tdc import tdc_hf_interface\n",
    "from tdc.model_server.tokenizers.scgpt import scGPTTokenizer\n",
    "import torch\n",
    "\n",
    "# Dataset de ejemplo, 36406 columnas y 4992 instancias\n",
    "# adata es un objeto de tipo AnnData(Annotated Data)\n",
    "adata = DataLoader(\"cellxgene_sample_small\",\n",
    "                   \"./data\",\n",
    "                   dataset_names=[\"cellxgene_sample_small\"],\n",
    "                   no_convert=True).adata\n",
    "\n",
    "# Estas dos primeras líneas cargan el modelo\n",
    "scgpt = tdc_hf_interface(\"scGPT\")\n",
    "model = scgpt.load()\n",
    "\n",
    "# Montamos la estructura de datos necesaria para los embeddings\n",
    "tokenizer = scGPTTokenizer()\n",
    "\n",
    "#Creamos un array con los nombres de los genes(id's)\n",
    "gene_ids = adata.var[\"feature_name\"].to_numpy()\n",
    "\n",
    "# Convierte los perfiles de transcripcion a tokens\n",
    "tokenized_data = tokenizer.tokenize_cell_vectors(\n",
    "    adata.X.toarray(), gene_ids)\n",
    "\n",
    "#Máscara para los valores de expresión iguales a 0\n",
    "mask = torch.tensor([x != 0 for x in tokenized_data[0][1]], #¿Por qué el primero siempre es un 0?\n",
    "                    dtype=torch.bool)\n",
    "\n",
    "# Embedding de la primera tupla (gene_id, count)\n",
    "# los embeddings son generados por medio del modelo\n",
    "# en este caso, sin hacer fine-tuning\n",
    "\n",
    "first_embed = model(tokenized_data[0][0], #Primer elemento de la primera tupla\n",
    "                    tokenized_data[0][1], #Segundo elemento de la primera tupla\n",
    "                    attention_mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tdc.model_server.models.scgpt.ScGPTModel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1129.32443367, 8124.38778062],\n",
       "       [6053.00411081, 3281.33693547],\n",
       "       [1427.00474023, 6600.4981705 ],\n",
       "       ...,\n",
       "       [5553.76652213, 7515.25536911],\n",
       "       [8307.49301103, 6732.16484772],\n",
       "       [1821.01498753, 6149.80761267]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Entendiendo el formato AnnData, Documentación oficial ###\n",
    "###                                                       ###\n",
    "\n",
    "# Todos los nombre de variables que salen son metadatos\n",
    "adata.obsm[\"spatial\"]\n",
    "\n",
    "# Representación de los datos como una matriz dispersa\n",
    "# La matriz tiene sus columnas para las observaciones y las variables, de forma diferenciada\n",
    "#adata.X\n",
    "\n",
    "# Acceso mediante índices\n",
    "#print(\"----- ID de las filas -----\")\n",
    "#print(adata.obs_names) # Transcriptomas(filas)\n",
    "#print(\"----- ID de las columnas -----\")\n",
    "#print(adata.var_names) # Genes(columnas)\n",
    "\n",
    "# Podemos visualizarlo como un DataFrame\n",
    "# Fíjate que hay columnas cuyos valores son \"nulos\" casi en su totalidad\n",
    "# .obs te devuelve información sobre las observaciones(transcritos)\n",
    "#adata.obs\n",
    "\n",
    "# Tanto .obs como .var tienen el formato Dataframe(se pueden convertir al de pandas)\n",
    "# .var te devuelve información sobre las variables(genes)\n",
    "#adata.var[\"feature_name\"].head(10)\n",
    "\n",
    "#Convertimos la matriz a un array de las expresiones\n",
    "#adata.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Qué hace el tokenizer?\n",
    "# tokenized_data es una lista de tuplas, de dimensión la cantida de obs de adata\n",
    "# cada elemento de la tupla es un tensor de pytorch(vectores numéricos)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El embedding se guarda en un diccionario\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "\n",
    "first_embed\n",
    "#embed_adata = AnnData()\n",
    "\n",
    "#embedding_2D = torch.stack(list(first_embed.values()))\n",
    "#embedding_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Representamos los embeddings usando scanpy\n",
    "\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "\n",
    "\n",
    "sc.pp.neighbors(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envTFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
